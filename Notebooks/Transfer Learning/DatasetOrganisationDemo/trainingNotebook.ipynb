{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "Demo of how to pull images in from dataset into jupyter notebook for training etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "\n",
    "#Paths\n",
    "train_dir = \"split_dataset/train/\"\n",
    "val_dir = \"split_dataset/val/\"\n",
    "test_dir = \"split_dataset/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the easiest way to load in the dataset as an iterator (as was performed in the Transfer Learning notebooks). Note that datagen.flow_from_directory is used to get the data (instead of datagen.flow which was used in the Transfer Learning notebooks). The iterators can be then used to fit/evaluate the model. \n",
    "\n",
    "Note that similar to the TransferLearning notebooks, I've set the batch size for val and test equal to the number of images in those sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 images belonging to 10 classes.\n",
      "Found 42 images belonging to 10 classes.\n",
      "Found 46 images belonging to 10 classes.\n",
      "Datasets loaded\n"
     ]
    }
   ],
   "source": [
    "#Count the number of images\n",
    "num_val_images = sum([len(files) for _, _, files in os.walk(val_dir)])\n",
    "num_test_images = sum([len(files) for _, _, files in os.walk(test_dir)])\n",
    "\n",
    "#Image Data Generators\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    #Do preprocessing and data augmentation\n",
    ")\n",
    "\n",
    "val_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    #Do preprocessing\n",
    ")\n",
    "\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    #Do preprocessing\n",
    ")\n",
    "\n",
    "#Load datasets\n",
    "train_iterator = train_datagen.flow_from_directory(train_dir, batch_size=30, class_mode='sparse')\n",
    "val_iterator = val_datagen.flow_from_directory(val_dir, batch_size=num_val_images, class_mode='sparse')\n",
    "test_iterator = test_datagen.flow_from_directory(test_dir, batch_size=num_test_images, class_mode='sparse')\n",
    "\n",
    "print(\"Datasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to get the X_train, y_train, X_test, y_test etc., as was done in Week 10 in the CNN digits notebook (e.g. where train_images, train_labels was used for fitting), then the below code will work. You would need to use OpenCV (Open Computer Vision) for images but not necessarily for numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport numpy as np\\nimport cv2  #For image processing - pip install cv2\\n\\n#Paths to split dataset\\ndataset_folder = \"split_dataset/\"\\nsplits = [\"train\", \"val\", \"test\"]\\n\\n#Function to load images and labels from a specific split\\ndef load_data(split_folder):\\n    X = []\\n    y = []\\n    flower_names = os.listdir(split_folder)  #Get flower types\\n    labels_map = {flower: idx for idx, flower in enumerate(flower_names)}  #Map flower names to integers\\n\\n    for flower in flower_names:\\n        flower_path = os.path.join(split_folder, flower)\\n        if not os.path.isdir(flower_path):\\n            continue\\n        for img_name in os.listdir(flower_path):\\n            img_path = os.path.join(flower_path, img_name)\\n            try:\\n                img = cv2.imread(img_path)  #Read the image\\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  #Convert BGR (OpenCV default) to RGB\\n                X.append(img)\\n                y.append(labels_map[flower])\\n            except Exception as e:\\n                print(f\"Error processing {img_path}: {e}\")\\n    \\n    return np.array(X, dtype=np.uint8), np.array(y, dtype=np.int8)\\n\\n#Load train, validation, and test datasets\\nX_train, y_train = load_data(os.path.join(dataset_folder, \"train\"))\\nX_val, y_val = load_data(os.path.join(dataset_folder, \"val\"))\\nX_test, y_test = load_data(os.path.join(dataset_folder, \"test\"))\\n\\n#Outputs\\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\\nprint(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\\nprint(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2  #For image processing - pip install cv2\n",
    "\n",
    "#Paths to split dataset\n",
    "dataset_folder = \"split_dataset/\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "#Function to load images and labels from a specific split\n",
    "def load_data(split_folder):\n",
    "    X = []\n",
    "    y = []\n",
    "    flower_names = os.listdir(split_folder)  #Get flower types\n",
    "    labels_map = {flower: idx for idx, flower in enumerate(flower_names)}  #Map flower names to integers\n",
    "\n",
    "    for flower in flower_names:\n",
    "        flower_path = os.path.join(split_folder, flower)\n",
    "        if not os.path.isdir(flower_path):\n",
    "            continue\n",
    "        for img_name in os.listdir(flower_path):\n",
    "            img_path = os.path.join(flower_path, img_name)\n",
    "            try:\n",
    "                img = cv2.imread(img_path)  #Read the image\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  #Convert BGR (OpenCV default) to RGB\n",
    "                X.append(img)\n",
    "                y.append(labels_map[flower])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(X, dtype=np.uint8), np.array(y, dtype=np.int8)\n",
    "\n",
    "#Load train, validation, and test datasets\n",
    "X_train, y_train = load_data(os.path.join(dataset_folder, \"train\"))\n",
    "X_val, y_val = load_data(os.path.join(dataset_folder, \"val\"))\n",
    "X_test, y_test = load_data(os.path.join(dataset_folder, \"test\"))\n",
    "\n",
    "#Outputs\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably easier to use the iterator method. Ok, let's test with a quick bit of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 images belonging to 10 classes.\n",
      "Found 42 images belonging to 10 classes.\n",
      "Found 46 images belonging to 10 classes.\n",
      "Datasets loaded\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from efficientnet import tfkeras as efficientnet  #EfficientNet from efficientnet.tfkeras\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Count the number of images\n",
    "num_val_images = sum([len(files) for _, _, files in os.walk(val_dir)])\n",
    "num_test_images = sum([len(files) for _, _, files in os.walk(test_dir)])\n",
    "\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "val_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "#Load datasets\n",
    "train_iterator = train_datagen.flow_from_directory(train_dir, batch_size=32, class_mode='sparse')\n",
    "val_iterator = val_datagen.flow_from_directory(val_dir, batch_size=num_val_images, shuffle=False, class_mode='sparse')\n",
    "test_iterator = test_datagen.flow_from_directory(test_dir, batch_size=num_test_images, shuffle=False, class_mode='sparse')\n",
    "\n",
    "print(\"Datasets loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup - same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = efficientnet.EfficientNetB4(\n",
    "    weights='noisy-student',\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "#Freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Add new custom top layers (this can be played with)\n",
    "model = keras.models.Sequential([\n",
    "    base_model,                          #Add the base model\n",
    "    layers.Dropout(0.2),                 #Add Dropout\n",
    "    #layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  #Add custom Dense layer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Train for up to 30 epochs and use the validation iterator for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "4/4 [==============================] - 53s 9s/step - loss: 2.2636 - accuracy: 0.1721 - val_loss: 2.0534 - val_accuracy: 0.3333\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - 22s 6s/step - loss: 1.9224 - accuracy: 0.3607 - val_loss: 1.8130 - val_accuracy: 0.5000\n",
      "Epoch 3/40\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1.7280 - accuracy: 0.5738 - val_loss: 1.6128 - val_accuracy: 0.5714\n",
      "Epoch 4/40\n",
      "4/4 [==============================] - 21s 6s/step - loss: 1.5743 - accuracy: 0.6066 - val_loss: 1.4562 - val_accuracy: 0.5952\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - 25s 7s/step - loss: 1.4275 - accuracy: 0.6885 - val_loss: 1.3263 - val_accuracy: 0.6429\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - 23s 6s/step - loss: 1.2327 - accuracy: 0.7213 - val_loss: 1.2221 - val_accuracy: 0.6905\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - 24s 6s/step - loss: 1.1285 - accuracy: 0.7705 - val_loss: 1.1278 - val_accuracy: 0.7143\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - 28s 7s/step - loss: 1.0449 - accuracy: 0.7705 - val_loss: 1.0502 - val_accuracy: 0.7381\n",
      "Epoch 9/40\n",
      "4/4 [==============================] - 30s 7s/step - loss: 0.9220 - accuracy: 0.8525 - val_loss: 0.9863 - val_accuracy: 0.7381\n",
      "Epoch 10/40\n",
      "4/4 [==============================] - 30s 8s/step - loss: 0.8897 - accuracy: 0.8525 - val_loss: 0.9327 - val_accuracy: 0.7143\n",
      "Epoch 11/40\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.8231 - accuracy: 0.8525 - val_loss: 0.8830 - val_accuracy: 0.7381\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.7678 - accuracy: 0.8607 - val_loss: 0.8339 - val_accuracy: 0.7857\n",
      "Epoch 13/40\n",
      "4/4 [==============================] - 30s 8s/step - loss: 0.7805 - accuracy: 0.8607 - val_loss: 0.7888 - val_accuracy: 0.7857\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.6844 - accuracy: 0.8770 - val_loss: 0.7482 - val_accuracy: 0.8333\n",
      "Epoch 15/40\n",
      "4/4 [==============================] - 34s 8s/step - loss: 0.6371 - accuracy: 0.8607 - val_loss: 0.7148 - val_accuracy: 0.8571\n",
      "Epoch 16/40\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.6149 - accuracy: 0.8934 - val_loss: 0.6900 - val_accuracy: 0.8333\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - 25s 7s/step - loss: 0.5904 - accuracy: 0.8770 - val_loss: 0.6654 - val_accuracy: 0.8333\n",
      "Epoch 18/40\n",
      "4/4 [==============================] - 26s 7s/step - loss: 0.5760 - accuracy: 0.9180 - val_loss: 0.6428 - val_accuracy: 0.8571\n",
      "Epoch 19/40\n",
      "4/4 [==============================] - 26s 7s/step - loss: 0.5643 - accuracy: 0.9016 - val_loss: 0.6187 - val_accuracy: 0.8571\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.4645 - accuracy: 0.9754 - val_loss: 0.5959 - val_accuracy: 0.8333\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - 29s 8s/step - loss: 0.4632 - accuracy: 0.9098 - val_loss: 0.5774 - val_accuracy: 0.8333\n",
      "Epoch 22/40\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.4383 - accuracy: 0.9344 - val_loss: 0.5614 - val_accuracy: 0.8333\n",
      "Epoch 23/40\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.4975 - accuracy: 0.8934 - val_loss: 0.5435 - val_accuracy: 0.8333\n",
      "Epoch 24/40\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.4431 - accuracy: 0.9426 - val_loss: 0.5259 - val_accuracy: 0.8571\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - 29s 7s/step - loss: 0.4079 - accuracy: 0.9426 - val_loss: 0.5110 - val_accuracy: 0.8571\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - 27s 6s/step - loss: 0.4088 - accuracy: 0.9590 - val_loss: 0.4993 - val_accuracy: 0.8571\n",
      "Epoch 27/40\n",
      "4/4 [==============================] - 24s 6s/step - loss: 0.4190 - accuracy: 0.9016 - val_loss: 0.4904 - val_accuracy: 0.8571\n",
      "Epoch 28/40\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.3975 - accuracy: 0.9590 - val_loss: 0.4826 - val_accuracy: 0.8571\n",
      "Epoch 29/40\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.3640 - accuracy: 0.9508 - val_loss: 0.4732 - val_accuracy: 0.8571\n",
      "Epoch 30/40\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.3320 - accuracy: 0.9672 - val_loss: 0.4643 - val_accuracy: 0.8571\n",
      "Epoch 31/40\n",
      "4/4 [==============================] - 30s 8s/step - loss: 0.3461 - accuracy: 0.9508 - val_loss: 0.4551 - val_accuracy: 0.8571\n",
      "Epoch 32/40\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.3458 - accuracy: 0.9590 - val_loss: 0.4472 - val_accuracy: 0.9048\n",
      "Epoch 33/40\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.3450 - accuracy: 0.9672 - val_loss: 0.4376 - val_accuracy: 0.9048\n",
      "Epoch 34/40\n",
      "4/4 [==============================] - 23s 6s/step - loss: 0.2969 - accuracy: 0.9672 - val_loss: 0.4280 - val_accuracy: 0.8810\n",
      "Epoch 35/40\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.3294 - accuracy: 0.9672 - val_loss: 0.4228 - val_accuracy: 0.8810\n",
      "Epoch 36/40\n",
      "4/4 [==============================] - 28s 7s/step - loss: 0.3169 - accuracy: 0.9344 - val_loss: 0.4199 - val_accuracy: 0.8810\n",
      "Epoch 37/40\n",
      "4/4 [==============================] - 26s 6s/step - loss: 0.2847 - accuracy: 0.9590 - val_loss: 0.4172 - val_accuracy: 0.9048\n",
      "Epoch 38/40\n",
      "4/4 [==============================] - 25s 6s/step - loss: 0.2931 - accuracy: 0.9590 - val_loss: 0.4096 - val_accuracy: 0.9048\n",
      "Epoch 39/40\n",
      "4/4 [==============================] - 22s 6s/step - loss: 0.2941 - accuracy: 0.9754 - val_loss: 0.4037 - val_accuracy: 0.9048\n",
      "Epoch 40/40\n",
      "4/4 [==============================] - 27s 7s/step - loss: 0.2880 - accuracy: 0.9508 - val_loss: 0.3962 - val_accuracy: 0.8810\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_iterator,\n",
    "    epochs=40,\n",
    "    validation_data=val_iterator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good - just had a play around if you had npz files instead:\n",
    "\n",
    "Won't be able to use flow_from_directory as that deals with images. Will need to do something else instead.\n",
    "\n",
    "If your data is organised a bit like the sorted_dataset that I created (instead of the split_dataset), might be able to use something like this Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flower names\n",
    "fNames = [\n",
    "    'phlox', 'rose', 'calendula', 'iris', 'leucanthemum maximum',\n",
    "    'bellflower', 'viola', 'rudbeckia laciniata', 'peony', 'aquilegia'\n",
    "]\n",
    "\n",
    "#Path to the dataset directory\n",
    "data_dir = \"...\"  #Replace with actual path\n",
    "\n",
    "#Map flower names to integer labels\n",
    "class_to_label = {name: idx for idx, name in enumerate(fNames)}\n",
    "\n",
    "#Initialize lists to store images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "#Go through each class subdirectory\n",
    "for fName in fNames:\n",
    "    cls_dir = os.path.join(data_dir, fName)\n",
    "    \n",
    "    #Ensure directory exists\n",
    "    if not os.path.isdir(cls_dir):\n",
    "        print(f\"Directory not found: {cls_dir}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    #List all files in the class directory\n",
    "    for file_name in os.listdir(cls_dir):\n",
    "        if file_name.endswith('.npz'):  #Check for .npz files\n",
    "            file_path = os.path.join(cls_dir, file_name)\n",
    "            \n",
    "            #Load the .npz file\n",
    "            data = np.load(file_path)\n",
    "            \n",
    "            #Assuming each file contains a key 'image'\n",
    "            images.append(data['image'])\n",
    "            labels.append(class_to_label[fName])  #Assign label based on the flower name - might need some checking\n",
    "            \n",
    "            data.close()\n",
    "\n",
    "#Convert lists to NumPy arrays\n",
    "X = np.array(images)\n",
    "Y = np.array(labels)\n",
    "\n",
    "#Split into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify \n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00cbf5791b9eca71d779a224797699885667845f7b0adeb3ee57eb7aff0a928f"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
